{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Identification Machine Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "South Africa is a multicultural society that is characterised by its rich linguistic diversity. Language is an indispensable tool that can be used to deepen democracy and also contribute to the social, cultural, intellectual, economic and political life of the South African society.\n",
    "\n",
    "The country is multilingual with 11 official languages, each of which is guaranteed equal status. Most South Africans are multilingual and able to speak at least two or more of the official languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a machine learning model that will take text which is in any of South Africa's 11 Official languages and identify which language the text is in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Import Python Librariies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Data Preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.utils import resample\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Model Building\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "# Model Evaluation\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "#from scikitplot.metrics import plot_roc, plot_confusion_matrix\n",
    "\n",
    "# Explore Data Analysis\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "style.use(\"seaborn-poster\")\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from matplotlib.pyplot import rcParams\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Import warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download libraries\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train_set.csv')\n",
    "test = pd.read_csv('data/test_set.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy = train.copy()\n",
    "test_copy = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_copy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_copy.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = train[\"lang_id\"].value_counts()\n",
    "value_counts.name = \"Raw Number\"\n",
    "\n",
    "value_normd = train[\"lang_id\"].value_counts(normalize=True)\n",
    "value_normd.name = \"Percentage\"\n",
    "\n",
    "display(pd.concat([value_counts, value_normd], axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(train[\"lang_id\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data among classes is balance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Techniques that we are going to use to clean our data**\n",
    "\n",
    "- Removing Noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Removing Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noise will be removed with the following steps.\n",
    "\n",
    "- Convert letters to lowercases\n",
    "- Remove punctuation\n",
    "- remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    to_del = [\n",
    "        r\"\\d+\",  # delete numeric values\n",
    "        r\"U+FFFD\",  # remove the \"character note present\" diamond\n",
    "    ]\n",
    "    for key in to_del:\n",
    "        text = re.sub(key, \"\", text)\n",
    "    \n",
    "    # strip punctuation and special characters\n",
    "    text = re.sub(r\"[,.;':@#?!\\&/$]+\\ *\", \" \", text)\n",
    "    # strip excess white-space\n",
    "    text = re.sub(r\"\\s\\s+\", \" \", text)\n",
    "    \n",
    "    return text.lstrip(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy['text'] = train_copy['text'].apply(cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32995</th>\n",
       "      <td>tsn</td>\n",
       "      <td>popo ya dipolateforomo tse ke go tlisa boetele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32996</th>\n",
       "      <td>sot</td>\n",
       "      <td>modise mosadi na o ntse o sa utlwe hore thaban...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32997</th>\n",
       "      <td>eng</td>\n",
       "      <td>closing date for the submission of completed t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32998</th>\n",
       "      <td>xho</td>\n",
       "      <td>nawuphina umntu ofunyenwe enetyala phantsi kwa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32999</th>\n",
       "      <td>sot</td>\n",
       "      <td>mafapha a mang le ona a lokela ho etsa ditlale...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      lang_id                                               text\n",
       "32995     tsn  popo ya dipolateforomo tse ke go tlisa boetele...\n",
       "32996     sot  modise mosadi na o ntse o sa utlwe hore thaban...\n",
       "32997     eng  closing date for the submission of completed t...\n",
       "32998     xho  nawuphina umntu ofunyenwe enetyala phantsi kwa...\n",
       "32999     sot  mafapha a mang le ona a lokela ho etsa ditlale..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_copy.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Splitting out the X variable from the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftrain = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ftrain['lang_id']\n",
    "X = ftrain['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Data tranformation with TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer( max_df = 0.9, min_df = 2, ngram_range = (1, 2))\n",
    "X_vectorized = vectorizer.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_df=0.9, min_df=2, ngram_range=(1, 2))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TfidfVectorizer(min_df=2,\n",
    "                                                max_df=0.9,\n",
    "                                                ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5.1 Splitting the training data into a training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val = train_test_split(X_vectorized,y,test_size=.2,shuffle=True, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Model Fitting and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 Logistic Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, max_iter=4000, multi_class='ovr', n_jobs=1,\n",
       "                   random_state=42)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticRegression(random_state=42,\n",
    "                                  multi_class='ovr',\n",
    "                                  n_jobs=1,\n",
    "                                  C=1e5,\n",
    "                                  max_iter=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>afr</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eng</th>\n",
       "      <td>0.995025</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997506</td>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nbl</th>\n",
       "      <td>0.994992</td>\n",
       "      <td>0.993333</td>\n",
       "      <td>0.994162</td>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nso</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996667</td>\n",
       "      <td>0.998331</td>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sot</th>\n",
       "      <td>0.996678</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998336</td>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ssw</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tsn</th>\n",
       "      <td>0.998333</td>\n",
       "      <td>0.998333</td>\n",
       "      <td>0.998333</td>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tso</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ven</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xho</th>\n",
       "      <td>0.994992</td>\n",
       "      <td>0.993333</td>\n",
       "      <td>0.994162</td>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zul</th>\n",
       "      <td>0.991653</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990826</td>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.997424</td>\n",
       "      <td>0.997424</td>\n",
       "      <td>0.997424</td>\n",
       "      <td>0.997424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.997425</td>\n",
       "      <td>0.997424</td>\n",
       "      <td>0.997423</td>\n",
       "      <td>6600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.997425</td>\n",
       "      <td>0.997424</td>\n",
       "      <td>0.997423</td>\n",
       "      <td>6600.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "afr            1.000000  1.000000  1.000000   600.000000\n",
       "eng            0.995025  1.000000  0.997506   600.000000\n",
       "nbl            0.994992  0.993333  0.994162   600.000000\n",
       "nso            1.000000  0.996667  0.998331   600.000000\n",
       "sot            0.996678  1.000000  0.998336   600.000000\n",
       "ssw            1.000000  1.000000  1.000000   600.000000\n",
       "tsn            0.998333  0.998333  0.998333   600.000000\n",
       "tso            1.000000  1.000000  1.000000   600.000000\n",
       "ven            1.000000  1.000000  1.000000   600.000000\n",
       "xho            0.994992  0.993333  0.994162   600.000000\n",
       "zul            0.991653  0.990000  0.990826   600.000000\n",
       "accuracy       0.997424  0.997424  0.997424     0.997424\n",
       "macro avg      0.997425  0.997424  0.997423  6600.000000\n",
       "weighted avg   0.997425  0.997424  0.997423  6600.000000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "logreg = LogisticRegression(C=1e5, multi_class='ovr',\n",
    "                                  n_jobs=1, random_state=42, max_iter=4000)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred_lg = logreg.predict(X_val)\n",
    "report = classification_report(y_val, y_pred_lg, output_dict=True)\n",
    "results = pd.DataFrame(report).transpose()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>afr</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eng</th>\n",
       "      <td>0.991736</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995851</td>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nbl</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.976667</td>\n",
       "      <td>0.988196</td>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nso</th>\n",
       "      <td>0.998333</td>\n",
       "      <td>0.998333</td>\n",
       "      <td>0.998333</td>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sot</th>\n",
       "      <td>0.996678</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998336</td>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ssw</th>\n",
       "      <td>0.998336</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999167</td>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tsn</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996667</td>\n",
       "      <td>0.998331</td>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tso</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ven</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xho</th>\n",
       "      <td>0.994958</td>\n",
       "      <td>0.986667</td>\n",
       "      <td>0.990795</td>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zul</th>\n",
       "      <td>0.973899</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.984336</td>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.995758</td>\n",
       "      <td>0.995758</td>\n",
       "      <td>0.995758</td>\n",
       "      <td>0.995758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.995813</td>\n",
       "      <td>0.995758</td>\n",
       "      <td>0.995759</td>\n",
       "      <td>6600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.995813</td>\n",
       "      <td>0.995758</td>\n",
       "      <td>0.995759</td>\n",
       "      <td>6600.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "afr            1.000000  1.000000  1.000000   600.000000\n",
       "eng            0.991736  1.000000  0.995851   600.000000\n",
       "nbl            1.000000  0.976667  0.988196   600.000000\n",
       "nso            0.998333  0.998333  0.998333   600.000000\n",
       "sot            0.996678  1.000000  0.998336   600.000000\n",
       "ssw            0.998336  1.000000  0.999167   600.000000\n",
       "tsn            1.000000  0.996667  0.998331   600.000000\n",
       "tso            1.000000  1.000000  1.000000   600.000000\n",
       "ven            1.000000  1.000000  1.000000   600.000000\n",
       "xho            0.994958  0.986667  0.990795   600.000000\n",
       "zul            0.973899  0.995000  0.984336   600.000000\n",
       "accuracy       0.995758  0.995758  0.995758     0.995758\n",
       "macro avg      0.995813  0.995758  0.995759  6600.000000\n",
       "weighted avg   0.995813  0.995758  0.995759  6600.000000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelstart = time.time()\n",
    "rf = RandomForestClassifier(max_features=4, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_val)\n",
    "report = classification_report(y_val, y_pred_rf, output_dict=True)\n",
    "results = pd.DataFrame(report).transpose()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3 Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>afr</th>\n",
       "      <td>0.996667</td>\n",
       "      <td>0.996667</td>\n",
       "      <td>0.996667</td>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eng</th>\n",
       "      <td>0.990083</td>\n",
       "      <td>0.998333</td>\n",
       "      <td>0.994191</td>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nbl</th>\n",
       "      <td>0.918966</td>\n",
       "      <td>0.888333</td>\n",
       "      <td>0.903390</td>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nso</th>\n",
       "      <td>0.984694</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sot</th>\n",
       "      <td>0.980165</td>\n",
       "      <td>0.988333</td>\n",
       "      <td>0.984232</td>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ssw</th>\n",
       "      <td>0.915683</td>\n",
       "      <td>0.905000</td>\n",
       "      <td>0.910310</td>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tsn</th>\n",
       "      <td>0.965232</td>\n",
       "      <td>0.971667</td>\n",
       "      <td>0.968439</td>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tso</th>\n",
       "      <td>0.996644</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.993311</td>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ven</th>\n",
       "      <td>0.989967</td>\n",
       "      <td>0.986667</td>\n",
       "      <td>0.988314</td>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xho</th>\n",
       "      <td>0.916239</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.904641</td>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zul</th>\n",
       "      <td>0.820433</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.850722</td>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.951515</td>\n",
       "      <td>0.951515</td>\n",
       "      <td>0.951515</td>\n",
       "      <td>0.951515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.952252</td>\n",
       "      <td>0.951515</td>\n",
       "      <td>0.951724</td>\n",
       "      <td>6600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.952252</td>\n",
       "      <td>0.951515</td>\n",
       "      <td>0.951724</td>\n",
       "      <td>6600.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "afr            0.996667  0.996667  0.996667   600.000000\n",
       "eng            0.990083  0.998333  0.994191   600.000000\n",
       "nbl            0.918966  0.888333  0.903390   600.000000\n",
       "nso            0.984694  0.965000  0.974747   600.000000\n",
       "sot            0.980165  0.988333  0.984232   600.000000\n",
       "ssw            0.915683  0.905000  0.910310   600.000000\n",
       "tsn            0.965232  0.971667  0.968439   600.000000\n",
       "tso            0.996644  0.990000  0.993311   600.000000\n",
       "ven            0.989967  0.986667  0.988314   600.000000\n",
       "xho            0.916239  0.893333  0.904641   600.000000\n",
       "zul            0.820433  0.883333  0.850722   600.000000\n",
       "accuracy       0.951515  0.951515  0.951515     0.951515\n",
       "macro avg      0.952252  0.951515  0.951724  6600.000000\n",
       "weighted avg   0.952252  0.951515  0.951724  6600.000000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(random_state=42 )    \n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dc = dt.predict(X_val)\n",
    "report = classification_report(y_val, y_pred_dc, output_dict=True)\n",
    "results = pd.DataFrame(report).transpose()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.4 Support vector machine Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelstart = time.time()\n",
    "svc = SVC(gamma = 0.8, C = 10, random_state=42)\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred_svc = svc.predict(X_val)\n",
    "report = classification_report(y_val, y_pred_svc, output_dict=True)\n",
    "results = pd.DataFrame(report).transpose()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.5 Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelstart = time.time() \n",
    "linsvc = LinearSVC()\n",
    "linsvc.fit(X_train, y_train)\n",
    "y_pred_lsvc = linsvc.predict(X_val)\n",
    "report = classification_report(y_val, y_pred_lsvc, output_dict=True)\n",
    "results = pd.DataFrame(report).transpose()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 F1 score evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistc regression F1 score\n",
    "\n",
    "logreg_f1 = round(f1_score(y_val, y_pred_lg, average='weighted'),4)\n",
    "\n",
    "# Random forest F1 score\n",
    "rf_f1 = round(f1_score(y_val, y_pred_rf, average='weighted'),4)\n",
    "\n",
    "# Decision tree F1 score\n",
    "dt_f1 = round(f1_score(y_val, y_pred_dc, average='weighted'),4)\n",
    "\n",
    "# Support Vector F1 score\n",
    "svc_f1 = round(f1_score(y_val, y_pred_svc, average='weighted'),4)\n",
    "\n",
    "# Linear Support Vector F1 score\n",
    "linsvc_f1 = round(f1_score(y_val, y_pred_lsvc, average='weighted'),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axis = plt.subplots(figsize=(10, 5))\n",
    "rmse_x = ['Logistic Regression','Random Forest Classifier','Support Vector Classifier','Linear SVC','Decision Tree Classifier']\n",
    "rmse_y = [logreg_f1,rf_f1,svc_f1,linsvc_f1,dt_f1]\n",
    "ax = sns.barplot(x=rmse_x, y=rmse_y,palette='winter')\n",
    "plt.title('Weighted F1-Score Per Classification Model',fontsize=14)\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Weighted F1-Score')\n",
    "for p in ax.patches:\n",
    "    ax.text(p.get_x() + p.get_width()/2, p.get_y() + p.get_height(), round(p.get_height(),4), fontsize=12, ha=\"center\", va='bottom')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Test Data tranformation with Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testx = test_copy['text']\n",
    "test_vect = vectorizer.transform(testx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rfc_pred_t = rf.predict(test_vect)\n",
    "# Multi-class Logistic Predict\n",
    "lmc_pred_t = logreg.predict(test_vect)\n",
    "#Decision Tree Predict\n",
    "dtc_pred_t = dt.predict(test_vect)\n",
    "# Support vector Machine Predict\n",
    "#svc_pred_t = grid_SVC.predict(test_vect)\n",
    "# Linear support vector Machine Predict\n",
    "#linsvc_pred = grid_LSVC.predict(test_vect)\n",
    "# AdaBoost Predict\n",
    "#ad_pred = ad.predict(test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linsvc_pred = grid_LSVC.predict(test_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Submitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['lang_id'] = lmc_pred_t\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['index','lang_id']].to_csv('testsubmission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All models that were trained performanced well with an accuracy of over 93%. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
